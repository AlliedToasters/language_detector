{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Wise Markov Chain Models for Language Detection\n",
    "This project is a response to the Fellowship.AI Language Detection challenge. The prompt is:\n",
    "\n",
    "\n",
    "> [European Parliament Proceedings Parallel Corpus](http://www.statmt.org/europarl/) is a text dataset used for evaluating language detection engines. The 1.5GB corpus includes 21 languages spoken in EU.  \n",
    "\n",
    "> Create a machine learning model trained on this dataset to predict the following [test set](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/language-detection/europarl-test.zip).\n",
    "\n",
    "## LSTM Aspirations\n",
    "\n",
    "I had been working with deep neural network models and really wanted to train up an LSTM. I was inspired by a paper published by Open AI, [Learning to Generate Reviews and Discovering Sentiment](https://arxiv.org/pdf/1704.01444.pdf), where the researchers train an unsupervised \"[Byte mLSTM](https://arxiv.org/pdf/1609.07959.pdf)\" model on Amazon reviews and the network [discovers sentiment on its own](https://blog.openai.com/unsupervised-sentiment-neuron/) (with no sentiment labels involved.) This was a fascinating result. I wanted to apply this technique to the Europarl corpus to build a language detector. Surely, if the Byte mLSTM can learn something as subtle as sentiment without supervision, it can learn to tell apart langauges.<br><br>\n",
    "I put together [some code](https://github.com/AlliedToasters/language_neurons) using this [mLSTM repository](https://github.com/guillitte/pytorch-sentiment-neuron) and rented a Google Cloud Compute instance (using my free $300 credit) to take advantage of a Tesla K80 GPU-enabled machine. I cleaned up the data a little bit and started training my mLSTM. This was Saturday night, and as of this writing it is now Wednesday and 84 hours have passed. The machine is roughly through 15 percent of A SINGLE EPOCH, even with full CUDA optimization with PyTorch. Perhaps I shouldn't be surprised, since the original researchers mention that \"training took one month across four NVIDIA Pascal GPUs...\" on a corpus larger than Europarl. The code on GitHub seems to suggest the training was completed in 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Think...\n",
    "With my LSTM chunking its way through the corpus, I had a lot of time to think. I wanted to simplify the model so it wouldn't take so long. Maybe something I could run on my laptop without GPUs.\n",
    "\n",
    "## Modeling Byte Sequences\n",
    "I spent some time learning about UTF-8 encoding (the encoding used in the Europarl corpus). Unlike ASCII, UTF-8 uses a variable number of bytes per character. It is reverse-compatible with ASCII, and reserves the ASCII bytes for a one-byte-per-character scheme. To expand its vocabulary to encompass all of Unicode, UTF-8 uses certain bytes (with decimal values above 128) to signal a coming byte chain that will encode a single unicode character.<br><br>\n",
    "As many of the languages in this corpus use unique characters, a model than can look at the sequences of bytes that appear in a given language will be powerful.\n",
    "\n",
    "## Markov Chains\n",
    "A Markov Chain model assumes that the probability of the next link of the series (in our case, the next byte) depends only on the previous links (the previous bytes.) The number of bytes in the model's \"memory\" is known as the \"order\" of the model.<br><br>\n",
    "This is a first-order Markov Chain model of the sentence, \"One fish two fish red fish blue fish.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![First-Order Markov Chain](https://cdn-images-1.medium.com/max/1600/1*UuD1-B7CFn3M2nUDy02sHg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the first-order model gives equal probability to the next state being \"two\", \"red\", \"blue\", or \\*END\\* when the current state is \"fish.\"<br><br>\n",
    "To improve the model, we can increase the order of the model to remember the state or states before the word Fish, giving us a way to model the order of the words \"two\", \"red\", and \"blue.\" (Thanks to [Alexander Dejeu and Hackernoon for the image](https://hackernoon.com/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71).)\n",
    "\n",
    "## Markov Chains for Classification\n",
    "Markov Chain models are usually used to predict the next character or word in a series. A famous example is the \"next word recommendation\" feature when writing a text message. These simple models work surprisingly well in this application, helping some users compose texts more quickly.<br><br>\n",
    "However, in this case, we are looking to classify chains of bytes as coming from a certain language. To do this, we just need to sum the probabilities of a set of pre-trained models moving along the byte sequence and apply the softmax function to the outputs values to get likelihoods for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "I will train with the Europarl corpus and validate with the set provided by Fellowship AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!mkdir data\\n!wget -P ./data http://www.statmt.org/europarl/v7/europarl.tgz\\n!tar -xzf ./data/europarl.tgz -C ./data/\\n!pip install pandas\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hide training data in git-ignored ./data directory because it's too big for GitHub.\n",
    "!mkdir data\n",
    "!wget -P ./data http://www.statmt.org/europarl/v7/europarl.tgz\n",
    "!tar -xzf ./data/europarl.tgz -C ./data/\n",
    "#Download and unzip validation data\n",
    "!wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/language-detection/europarl-test.zip\n",
    "!unzip europarl-test.zip\n",
    "#Install dependencies\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from markov_model import update_progress\n",
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding language bg...\n",
      "adding language cs...\n",
      "adding language da...\n",
      "adding language de...\n",
      "adding language el...\n",
      "adding language en...\n",
      "adding language es...\n",
      "adding language et...\n",
      "adding language fi...\n",
      "adding language fr...\n",
      "adding language hu...\n",
      "adding language it...\n",
      "adding language lt...\n",
      "adding language lv...\n",
      "adding language nl...\n",
      "adding language pl...\n",
      "adding language pt...\n",
      "adding language ro...\n",
      "adding language sk...\n",
      "adding language sl...\n",
      "adding language sv...\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe to manage files and labels.\n",
    "cols = ['lang', 'path']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "languages = os.popen('ls ./data/txt/').read().split('\\n')[:-1]\n",
    "for lang in languages:\n",
    "    print('adding language {}...'.format(lang))\n",
    "    lang_frame = pd.DataFrame(columns=cols)\n",
    "    txts = os.popen('ls ./data/txt/{}/'.format(lang)).read().split('\\n')[:-1]\n",
    "    paths = ['./data/txt/{}/{}'.format(lang, txt) for txt in txts]\n",
    "    lang_frame['path'] = paths\n",
    "    lang_frame['lang'] = lang\n",
    "    lang_frame.index = range(len(df), len(df)+len(lang_frame))\n",
    "    df = pd.concat([df, lang_frame], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelled Text\n",
    "To simplify training, I combine all of the texts for each language into a single .txt file located at ./data/{language}.txt. I remove some html tag-looking artifacts from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering data:  bg\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  cs\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  da\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  de\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  el\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  en\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  es\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  et\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  fi\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  fr\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  hu\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  it\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  lt\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  lv\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  nl\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  pl\n",
      "Progress: [################---------] 64.619% problem decoding:  140993 pl\n",
      "Progress: [#########################] 99.989% gathering data:  pt\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  ro\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  sk\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  sl\n",
      "Progress: [#########################] 100% Done...\n",
      "gathering data:  sv\n",
      "Progress: [#########################] 100% Done...\n"
     ]
    }
   ],
   "source": [
    "def get_text(idx, df):\n",
    "    \"\"\"Takes idx and df and returns text.\"\"\"\n",
    "    path = df.loc[idx].path\n",
    "    with open(path, 'r') as f:\n",
    "        result = f.read()\n",
    "    return result\n",
    "\n",
    "def remove_tags(input_string):\n",
    "    \"\"\"Removes html tag-like artifacts in the data.\"\"\"\n",
    "    result = input_string\n",
    "    tag = re.compile(r'<[^<]*>')\n",
    "    while tag.search(result):\n",
    "        match = tag.search(result)\n",
    "        strt = match.span()[0]\n",
    "        stp = match.span()[1]\n",
    "        result = result[:strt] + result[stp:]\n",
    "    return result\n",
    "\n",
    "for lang in df.lang.unique():\n",
    "    cnt = 0\n",
    "    full_text = ''\n",
    "    lng_df = df[df.lang == lang]\n",
    "    print('gathering data: ', lang)\n",
    "    for i, row in lng_df.iterrows():\n",
    "        try:\n",
    "            txt = get_text(i, lng_df)\n",
    "        except UnicodeDecodeError:\n",
    "            print('problem decoding: ', i, lang)\n",
    "            continue\n",
    "        txt = remove_tags(txt)\n",
    "        full_text += txt\n",
    "        cnt += 1\n",
    "        prog = cnt/len(lng_df)\n",
    "        update_progress(prog)\n",
    "    with open('./data/{}.txt'.format(lang), 'w+', encoding='utf-8') as f:\n",
    "        f.write(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there is one decoding error in the language \"pl.\" Since this problem occurs just once, I will omit the text from that file from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Software\n",
    "It was surprisingly hard to find software to apply this model. I ended up coding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
